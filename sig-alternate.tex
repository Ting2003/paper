\documentclass{sig-alternate}
\usepackage{enumerate}
\usepackage{verbatim}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{rotating}
\usepackage{cite}
\usepackage{multirow}
\def\thesection{\arabic{section}}
\graphicspath{{./figures/}}

\begin{document}
\title{A New Parallel Method for Power Grid Simulation Based on Distributed Memory System}

%\author{Ting Yu, Zigang Xiao, Martin.D.F. Wong}
%	{Depart of Electrical and Computer Engineering, UIUC\\
%	Champaign IL 61820\\
%	Email: tingyu1@illinois.edu, xxxxxxxxx, mfwong@illinois.edu}
%}
\maketitle

\begin{comment}
This paper presents an efficient parallel Domain Decomposition method for large-scale power grid simulation. 
Based on multi-core platform, a new data storage strategy is proposed to overcome the memory bottleneck of traditional methods. 
Techniques as 3D irregular grid friendly overlapping, via detection as well as grouping techniques are utilized to accelerate the 
simulation. A new communication strategy is proposed and exhibits minimum communication overhead. Experimental results on several 
industrial and large-scale benchmarks show that the proposed method achieves more than 110X speedup over a state-of-the-art direct 
LU solver. Power grid containing over 190M nodes could be solved within 5 minutes.
\end{comment}

\begin{abstract}
Due to the development of CMOS process technology, the size of power grids is becoming larger and larger. There are usually 
multi-million nodes in power grids. Analyzing the large size power grid has become very expensive in both time and memory. This paper 
presents an efficient parallel iterative Additive Schwarz Method (ASM) for large-scale power grid simulation. Based on distributed 
memory system, a new 
data storage method is proposed to overcome the memory bottleneck of traditional methods. Much larger problem can be processed. 
Techniques including overlapping for 3D irregular grid, via detection and grouping are utilized to accelerate the 
simulation. Moreover, a new communication strategy exhibiting minimum communication overhead is proposed. Experimental results on 
several 
industrial and large-scale benchmarks show that the proposed method achieves more than 110X speedup over a state-of-the-art direct 
LU solver. It is the first time reported that power grids that contain over 190M nodes are successfully solved within 5 minutes.	
\end{abstract}

\section{Introduction}
Power grid analysis is an important process for signal integrity and further design 
optimization. To shorten the design cycle and cost, power grid simulation should be finished within a reasonable amount of time. 
However, with the fast increase of VLSI circuit scale, power grids usually contains millions or even hundreds of millions 
of nodes. The large problem size has made efficiently analyzing power grids a very challenging task. It will be prohibitively 
expensive for traditional solvers such as direct LU to solve the problem. Besides, traditional methods may suffer from memory 
bottleneck, which makes them incapable to process large size problems. Efficient solvers are strongly desired to overcome these problems.
%\begin{comment}
	\begin{figure}[htbp]
	  \subfigure[][2D view of power grid]{
		\includegraphics[width=0.2\textwidth]{2D_pg_1_new.pdf}\label{subfig.1}}
	  \subfigure[][3D view of power grid]{
		\includegraphics[width=0.25\textwidth]{3D_pg_new.pdf} \label{subfig.2}}
	  \caption{Simple model of power grid: 
	  \subref{subfig.1} 2D view
	  \subref{subfig.2} 3D view}
	  \label{pg_model}
	\end{figure}
Figure~\ref{pg_model} shows a two-layer simplified power grid model for DC analysis. Vias are used to 
	connect different layers. It is well-known that analyzing a power grid is equivalent to solve a linear system, which is 
%\end{comment}
	\begin{equation}\label{eq1}
		\textbf{Gu=I}
	\end{equation}
In the above equation, {$\textbf{G}$} is the system conductance matrix, $\textbf{u}$ and $\textbf{I}$ are respectively voltage vector and current vector.

	Because power grids are usually in large size, more and more time will be
	spent for simulation. When the size of problem is large enough, the simulation time will be intractable. This is true even
	for a efficient serial solver.
	Besides, the memory bottleneck is also a problem. There will not be enough memory to hold the large size data. To address the
	simulation time issue, parallel methods should be taken into consideration. For example, with "divide and
	conquer" based Domain Decomposition method, a lot of parallelism can be introduced during the simulation process. Furthermore,
	parallel architectures can be utilized to overcome the memory bottleneck. For example, in distributed memory system, more 
	processors will bring bigger memory size. Thus, parallel method is a 
	promising approach to large-scale power grid analysis. 

	There are already several published works on analyzing power grid with parallel methods, including multigrid on 
	GPU\cite{Zhuofeng}, parallel direct LU 
	solver\cite{Super_LU_website} and public parallel domain decomposition method\cite{PETSC_website, kaisun, voronov}. Although 
	GPU-based multigrid method\cite{Zhuofeng} is very fast, large error may be
	introduced in final solution due to the interpolation and restriction
	process, especially for ill-conditioned systems. SuperLU\_MT and SuperLU\_DIST are two parallel direct LU 
	solvers developed in \cite{Super_LU_website}. SuperLU\_MT is 
	based on shared memory system. Limited by the number of threads, not too much speedup can be gained. 
	It is reported to be only 5X to 10X faster than commercially popular solvers\cite{Super_LU_website}. Besides, the largest 
	problem that can be solved by this method is limited by the system memory of a single processor or core. SuperLU\_DIST 
	is developed based on distributed memory system. Master processor is used to read in the whole grid and build a global 
	matrix, then distribute the subdomain matrices to other processors. 
	There is an obvious disadvantage of this method: the master processor wastes a lot of memory to store irrelevant data, because 
	it only needs to solve one subdomain. Hence, the largest problem that can be handled by this method can not be very large. In 
	\cite{Super_LU_website}, the largest problem size that can be solved by SuperLU\_DIST is reported to be two-million.

	Domain decomposition is very suitable for parallelization, since there are many independent subdomains that can be
	solved in parallel at each iteration. There are two types of Domain Decomposition method, including Schur Complement Method (SCM) 
	and Additive Schwarz Method (ASM). Schur Complement Method produces a dense matrix that is time consuming to form and solve. It
	is not suitable for large-scale power grid analysis. On the other hand, Additive Schwarz Method will not introduce this problem 
	and is fit for the power grid analysis. PETSC\cite{PETSC_website} is an implementation of ASM.
	However, there are several limitations. First, PETSC can only handle 2D regular grid. 
	The overlapping technique can not be used for 3D irregular grid. Another problem is that since a master processor is 
	still required to store the whole grid information, there will be memory bottleneck as well. The same problem exists in 
	\cite{kaisun, voronov}. In this paper, because of the newly developed data storage approach, the method proposed in this paper 
	will not suffer the memory bottleneck anymore.
	
	This paper proposes an efficient parallel implementation of ASM. A new data storage strategy is developed to overcome the 
	memory problem. Much 
	larger size of power grid can be handled. Techniques are utilized to accelerate the simulation process. A lot of 
	speedup is gained over a state-of-art direct LU solver. The main contributions include:
	\begin{enumerate}[1)]
	\item Based on distributed memory system, a new data storage method is proposed to eliminate the memory bottleneck of
	the above mentioned methods. Very large-scale power grid size can be efficiently handled.
	\item An overlapping techinique for 3D irregular grid is proposed to allow faster convergence. 
	\item A new data communication strategy is proposed. It achieves minimum communication overhead.
	\item A via detection technique is presented to efficiently reduce the size of conductance matrix without any loss of accuracy.
	\item A grouping technique is introduced to further increase parallelism.
	\end{enumerate}

	The rest of this paper is organized as follows. Section~2 gives an overview of ASM. Section~3 presents 
	the proposed parallel ASM based on a distributed memory system. Experimental results are illustrated in section~4. 
	Finally, conclusions are made in section~5.
%\begin{comment} 

\section{Overview of Additive Schwarz method (ASM)}	
	Additive Schwarz method (ASM) is a type of domain decomposition. It solves a boundary value problem for a partial 
	differential equation approximately by splitting it into boundary value problems on smaller domains and adding the results.
	There are two types of ASM: Algebraic ASM and geometric ASM. 
	Figure~\ref{Fig2}\subref{subfig.1} shows the algebraic ASM method utilized in \cite{kaisun}. The global matrix
	is partitioned into several submatrices in row direction. Jacobi or Gauss-Seidel method is adopted to iterate among 
	submatrix domains until convergence is reached.
	\begin{figure}[htbp]
	  \subfigure[][Algebraic ASM]{
		\includegraphics[width=0.22\textwidth]{AASM_new.pdf}\label{subfig.1}}
	  \subfigure[][Geometric ASM]{
		\includegraphics[width=0.22\textwidth]{GASM_new.pdf} \label{subfig.2}}
	  \caption{Two types of ASM: 
	  \subref{subfig.1} Algebraic ASM
	  \subref{subfig.2} Geometric ASM}
	  \label{Fig2}
	\end{figure}
	Figure~\ref{Fig2}\subref{subfig.2} illustrates the geometric ASM utilized in \cite{Zhongyu}. Subdomains are partitioned based on
	geometrical information. Each subdomain then forms its local matrix. The matrix is
	solved in the same way as Algebraic ASM. In both the two types of ASM, overlapping technique can be introduced, such as 
	Figure~\ref{Fig2}. However, the characteristics of overlapping are different.
	In algebraic ASM, overlapping can only happens between a subdomain and one neighboring subdomain. While in 
	geometric ASM, overlapping can be introduced between one subdomain and all its 8 neighboring subdomains.
	
%\end{comment}	
	It is known that introducing overlapping between different subdomains leads to faster convergence, 
	since overlapping reduces the system conductance matrix eigenspectrum\cite{Klawonn, Taopeng}. More overlapping leads to faster 
	convergence.
	But the equivalent subdomain size is increased as well, which will deteriorate the performance. As a result, there is a 
	balance point of overlapping ratio to gain the optimal performance.

	There are already works talking about parallelization of overlapping ASM, such as\cite{kaisun}. The are several problems in 
	\cite{kaisun}. First one is that since each processor holds all the nodes' value of the whole grid, the memory bottleneck
	is not alleviated. Second, the communication strategy is not efficient: After each iteration, all the
	nodes' value will be gathered from all processors into the master processor. The master processor then broadcasts all the nodes'
	value back to all processors. Much more
	data than necessary is transferred. In fact, each processor only needs to exchange its boundary nodes' value with neighboring 
	subdomains. However, because of overlapping and 3D irregular grid structure, it is a challenging problem. Section 3
	gives a detail illustration of the situation.
 
%\begin{comment}
	Besides, SOR, PCG, MG (multigrid) methods can be combined with ASM to accelerate convergence. However, there are limitations of 
	the resulting
	methods. In SOR method, $w$ is used to accelerate convergence. However, the optimal $w$ value depends on system matrix 
	eigenvalues. It is hard to choose the optimal $w$ for large
	size problem, and a bad $w$ value can even lead to divergence; MG may have
	error or even convergence problem, especially for 3D irregular grid; In PCG, a global
	matrix is required, hence the maximum problem size that can be solved is limited.
%\end{comment}
\section{Parallel implementation on \\distributed memory system}
  \subsection{Parallell system}
	There are mainly two types of parallel architectures: shared memory and distributed memory system. In shared 
	memory system, 
	multiple cores share common address space. There is no overhead for data communication. However, both the number
	of threads and memory size are limited. In distributed 
	memory system, a lot of processors are clustered together to form a local network. Each processor has independent memory. 
	Extra time is needed for data communication. Considering the large problem 
	size and efficient simulation time requirement, distributed memory system is adopted in this paper. The implementation is based
	on widely utilized message-passing library (MPI).
  \subsection{Grid Partitioning}
	In \cite{Zhongyu}, 1D partitioning is thought to be in better performance than 2D partitioning. However, the comparison is not 
	fair, since the number of elements in 1D partitioned subdomain and 2D partitioned subdomain is not equal. Here we prove that
	for parallel consideration, 2D partitioning presents a better performance than 1D partitioning. 

%\begin{comment}	
	\begin{figure}[htbp]
	  \subfigure[][1D partitioning]{
		\includegraphics[width=0.22\textwidth]{1D_partition_new.pdf}\label{subfig.3.1}}
	  \subfigure[][2D partitioning]{
		\includegraphics[width=0.22\textwidth]{2D_partition_new.pdf}\label{subfig.3.2}}
	  \caption{2 types of partitioning:
	  \subref{subfig.3.1} 1D partitioning
	  \subref{subfig.3.2} 2D partitioning }
	  \label{Fig3}
	\end{figure}
	Figure~\ref{Fig3} lists the two types of partitioning. For fairness, each subdomain in 1D partitioning contains the 
	same number of nodes with 2D partitioning. As a result, the total number of subdomains for 1D partitioning and 2D partitioning are the same. For 
	each subdomain 
	in Figure~\ref{Fig3}\subref{subfig.3.2}, assume  it is square and each side contains $k$ nodes. The total 
	number of nodes' value needed to be exchanged with neighboring subdomains is $4k$. For a subdomain in 
	Figure~\ref{Fig3}\subref{subfig.3.1}, each side contains
	$m_2k$ nodes. As a result, $2m_2k$ nodes' value are needed to be exchanged with its left and right 
	neighboring subdomains. Equation\eqref{eq2} illustrates the communication cost relationship between these two types of
	partitioning. $m_2>2$ is generally always true in most cases, thus 2D partitioning has more favorable surface-to-volume ratio.    
	\begin{equation}
		2m_2k > 4k, \hspace{0.5 in} m_2 > 2 \label{eq2}
	\end{equation}
%\end{comment}
	Besides, 2D partitioning presents a better scalability than 1D partitioning. This can be
	seen from the isoefficiency expression of 2 cases. Isoefficiency represents the scalability of algorithm
	with input size. The lower order of isoefficiency expression, the better scalability. Isoefficiency functions for 1D 
	and 2D partitioning are respectively $\Theta(p^2)$ and $\Theta(p)$\cite{Grama, Kumar}, where p is the number of processors. 
	For given number of processors, far smaller size of problem is required for 2D partitioning to achieve the given efficiency.

	Based on above considerations, 2D partitioning is utilized in this paper. 
  \subsection{Data storage}
	In traditional distributed memory system based domain decomposition method, input data is first loaded into the master processor. 
	The master processor distributes the data to others. The master processor spends a lot of extra memory to store the whole grid 
	information. As a result, the maximum problem size that can be solved is limited. In this paper, we propose a new data storage 
	method to avoid the memory bottleneck. With this method, very large-scale problem size can be processed.

	According to the number of subdomains and the grid's geometrical information, each subdomain's boundary information can be 
	extracted. With this information, input netlist is divided into several different files, where each file maintains all the net 
	information of a subdomain. This process is finished by the master processor. Then, each processor parses data from its
	corresponding file and builds up its local matrix simultaneously.

	Though writing I/O process is slow, the parsing stage, which is the main time consuming part and is 
	processed by a single processor in sequential method, can now be executed in parallel. As a result, 
	there will not be much extra overhead with this strategy.
     \subsection{Communication overhead}
	The formed local matrices are solved by direct solvers. Jacobi iteration method is used
	to update different subdomains until convergence. In the end of each iteration, each subdomain needs to update its 
	boundary nodes' value. With overlapping, there are two types of boundary nodes: Nodes that do no 
	belong to a subdomain, but are connected to internal nodes of 
	this subdomain; nodes that belong to a subdomain and are boundary nodes of neighboring subdomains. The 
	two types of boundary nodes are illustrated in Figure~\ref{comm}. In the rest of this paper, the first type of nodes are defined
	as ``outer boundary nodes", while second are defined as ``inter boundary nodes". 

	An intuitive strategy for sending and collecting boundary information is illustrated as follows:

	\begin{enumerate}[1.]
	  \item Each subdomain sends its inter boundary nodes' value sequentially to 8 neighboring subdomains;
	  \item The subdomain then collects its outer boundary nodes' value sequentially from 8 neighboring subdomains.
	\end{enumerate}	

 	However, this strategy is not efficient in message passing system. The reason can be seen from analyzing the
	communication model:
	\begin{equation}
		T_{msg} = t_s + t_wL\label{eq_msg}
	\end{equation}
	$t_s$ is the start up time, $t_w$ is the incremental transfer time per word, and $L$ is the length of message in words.
	
	Generally, $t_s$ is tens of thousands larger than $t_w$. It is highly preferable to send less 
	large messages rather than many small messages to reduce the start up time. Since the total number of boundary nodes is small, 
	time spend on $t_wL$ is short. For a subdomain, $8\times 2 = 
	16$ sending or receiving operations are required. As a result, in each iteration, $16t_s$ start up time is
	needed for a subdomain to finish the data transfer. However, the start up time for the whole grid is longer.
	This is because the above process will be partly serial between different subdomains, because some subdomains need to wait until
	its neighbors finish the sending or receiving operations. For example, 
	assume there are only two neighboring subdomains in the grid: ``A" and ``B". ``A" needs to send data to ``B", while ``B" needs
	to send data to ``A". ``B" can not start until ``A" finish its operation. The serial process results in 
	longer start up time. In 2D partitioned power grids, there are much more neighboring subdomains. This leads to much 
	longer start up time. The worse thing is that the control flow is much more complicated, since each subdomain have more 
	neighboring subdomains involved in sending and receiving data. Furthermore, even the control flow is realized, for a large size of
	power grid, a lot of iterations may be required before reaching convergence. As a result, the total start up time should not be 
	ignored. The above problems can be overcame by the new strategy presented in the following paragraphs.

%\begin{comment}
	\begin{figure}[htbp]
	  \centering
	  \includegraphics[width=0.45\textwidth]{communication_1_new.pdf}
	  \caption{Distribution of boundary nodes}
	  \label{comm}
	\end{figure}
%\end{comment}

	Figure~\ref{comm} shows the distribution of boundary nodes along vertical direction. Plain blocks refer to subdomains without 
	overlapping, while shadowed blocks refer to subdomains with some extent of overlapping. There are several difficulties in 
	developing an efficient communication strategy with this model. As we can see, net connection can be irregular. Further, 
	as power grid is in multilayer 
	structure, the inter and outer boundary nodes will appear at the same or different positions
	among different layers. Moreover, because of overlapping, there can be multiple solutions for a node's value in the boundary 
	overlapping area, since this node belongs to several subdomains at the same time. For convergence consideration, only one value 
	should be used to update subdomains. Here we use the node's value from the subdomain that contains the node and has the biggest 
	index.

	It is observed that geometrical information can be utilized to overcome the above difficulties. For each processor, we allocate 
	a local inter and outer boundary array, to store the boundary nodes' value. Based on the geometrical 
	characteristic of inter and outer boundary nodes, we divide each type of nodes into 8 node sets. Each node set represents the
	geometrical relationship between two neighboring subdomains. For example, ``south east" node set in the outer boundary of 
	subdomain 5 stores a inter boundary node set's value of subdomain 3. We define this node set as ``south east" node set in inter
	boundary array of subdomain 3. This is because subdomain 3 is to the ``south east" of subdomain 5. Based on this relationship,
	it is easy to locate the boundary node set from neighboring subdomains. Each array stores 8 corresponding node sets' values in 
	the increasing order of neighboring subdomains' index.  Boundary nodes within 
	these node sets are sorted according to their 3D coordinates. As a result, it is insensitive to 3D irregular structure. With the
	geometrical relationship and coordinates, it is easy to access a boundary node's value from neighboring subdomains. 

	After each iteration, each subdomain has new solutions for all internal nodes. The inter 
	boundary nodes' value will then be copied into local inter boundary array. Processor ``0" gathers these arrays from each subdomain
	into a global inter boundary array owned by processor ``0". Then, this array will be reordered to generate the global outer 
	boundary array. This array
	contains all the outer boundary nodes' value for each subdomain. 

	The reordering process is shown in Figure~\ref{Fig4}. Labels in global inter boundary array indicates the subdomain index, where
	the local inter boundary array of corresponding subdomain is stored. Each local inter boundary array stores 8 node sets' values. 
	Labels within the local inter boundary arrays represents ``<subdomain\_id>\_
	<boundary\_node\_set\_direction>".

	To gather all the updated outer boundary nodes' values of a subdomain, each outer boundary node set's values needs to be located
	and copied from corresponding internal boundary node set position of neighboring subdomains. For example, the 
	outer boundary array of subdomain 5 includes the ``north west" inter boundary node set of subdomain 1, ``north" inter boundary 
	node set of 
	subdomain 2, and so on. We use processor 0 to scan a subdomain's neighboring subdomains by increasing index and locates the
	values of the 
	local inter boundary node set. These values are copied into corresponding places in global outer boundary array. The 
	reordered global array will then be scattered to all processors, so that each processor has its outer boundary nodes' value 
	ready for later operation. 
%\begin{comment} 
	\begin{figure}[htbp]
	  \centering
	  \includegraphics[width=0.45\textwidth]{reordering_new.pdf}
	  \caption{Reorder inter boundary array into outer boundary array}
	  \label{Fig4}
	\end{figure}
%\end{comment}

	After a subdomain receiving its local outer boundary array, the values are assigned to corresponding internal boundary 
	nodes. As 
	mentioned before, nodes in overlapping area have several different values from overlapping subdomains. For these nodes, 
	as their values are assigned according to the ascending order of neighboring subdomain, the new value will always cover old one. 
	After the assigning process, the node's value is from neighboring subdomain with the largest index that contains this node.  
	
	In the above method, only one gathering and one scattering operations are needed in each iteration. As a result, the start up time 
	is only $2t_s$, which is much less than the previous one. In fact, it is also the minimum 
	start up time for this problem. Since the total number 
	of boundary nodes are always very small, combining with the fact that $t_w$ is very small, it can be concluded that the 
	time for transferring messages is negligible. 

	In conclusion, the proposed strategy successfully achieves minimum communication overhead. It is insensitive to 3D irregular 
	grid structure, and also effectively overcomes the difficulties caused by overlapping.	
  \subsection{Via detection technique}
	As mentioned before, power grid nodes are vertically connected by vias. If the vias has no 
	resistance, it is called ``shortened" vias, and those nodes always have identical voltage value. Because of this relationship, 
	only stamping the representative
	node into the system conductance matrix is an effective method to reduce the matrix size. In extreme case, if all the
	vias are shortened, 3D power grid can be modeled as a 2D grid without any loss of accuracy. The resulting matrix size
	is much smaller, a lot of simulation time can thus be saved. 
 
	It is also observed that if a via has very small resistance value, it can also be treated as ``shortened" one 
	without any loss of accuracy. We set a threshold value for via nets. Via nets 
	whose values are below this threshold will be substituted with ``shortened" nets. Without loss of accuracy, this threshold value 
	is experimentally tuned to be 1e-5. 
  \subsection{Grouping technique} 
	Power grid usually contains several physically disjoint components, for example, VDD and GND sub-circuit. With multi-voltage 
	technique, more components can be included, for example, VDD2 circuit. Since these components are totally independent, they can
	be solved in parallel. We utilize the group collective 
	communication technique to implement this idea. Processors are grouped into several clusters, where each cluster 
	simulates one component. Both the parsing and later stage processes can be performed in parallel. 
	If all the components are similar in size, half or even more runtime can instantly being saved. 
	It should be noticed that the time saving comes at a cost of consuming more processors. If $p$ processors are used to 
	solve different components sequentially, for a benchmark of $n$ components, the grouping technique requires $np$ processors to
	do the simulation.  
\section{Numerical Experiments}
	We implement a direct LU solver: UMFPACK on a single CPU, and the proposed ASM on distributed memory system. 
	All codes are written in C++ and compiled with mpicxx under Linux system. 
  \subsection{Simulation Environment}
	The single CPU is with 2.67 GHz frequency and 24 GB RAM. The parallel platform is composed of a set of cluster 
	computing nodes, each with two 2.67 GHz Intel Xeon hex-core processors. Each node has in total 24 GB of RAM, with about 
	1.8 GB memory per processor. Message passing package MPICH2 is used.  
  \subsection{Implementation}
	Error tolerance is set to be $10^{-5}$ for all test cases in order to meet the following accuracy requirement:
	\begin{equation}
		Max(e) < 0.01 mv, Avg(e) < 0.1 mv\\ \label{eq_err}
	\end{equation}
	In the experiment, overlapping ratio is defined to be the ratio between half of the overlapping length to the total side length of 
	a subdomain. To achieve a good balance between iteration numbers and matrix size, overlapping ratio is experimentally 
	settled to be 0.2.
			
	Three sets of benchmarks are used for comparison, including 
	IBM power grid benchmarks pg3 to pg6, x250, x200, y250, y200 and 4 artificially produced benchmarks. pg3 to pg6 are 
	small size wire bond type benchmarks with vias, x250, x200 and y250, y200 are respectively two median C4 type benchmarks with
	shortened vias or regular vias. The 4 large-scale benchmarks are all C4 type grids with vias. Simulation results are 
	listed in Table \ref{tb_p}, \ref{tb_v}, \ref{tb_s}, \ref{tb_m}, and \ref{tb_l}.
	\begin{table}[h]
	  \include {./tables/table_parse} \label{tb_p}
	  \include{./tables/table_viagroup} \label{tb_v}
	%\end{table}	
	%\begin{table}[h]
  	  \include {./tables/table_small} \label{tb_s}
	\end{table}
	\begin{table}[h]
  	  \include{./tables/table_median} \label{tb_m}
	%\end{table}
	%\begin{table}[h]
  	  \include{./tables/table_large} \label{tb_l}
	\end{table}
	The meanings of items in the tables are as follows: Ckt: name of testing power grid; \#N: total number of nodes in the circuit; 
	\#B: number of blocks; \#C: number of disjoint components; \#Rep: number of representative nodes; 
  	$t_N$: simulation time with original nodes; $t_{Rep}$: simulation time with representative nodes; 
	$t_g$: simulation time with representative nodes and grouping technique; $t_c$: simulation time for direct LU solver. 
	$t_p$: simulation time with proposed method in this paper.  
	All $t_c$ is obtained by solving the circuits with independent components. All 
	simulation time includes both grid set up session and solve session. All simulation time units are second.

	Table \ref{tb_p} shows the results of parsing with single cpu and multi processors with grouping technique. We can see that the
	parallel parsing cost only a little more time than serial one. Consider the great amount of time saving in the solving process,
	it is worthy to pay for the cost. 
	Data in Table \ref{tb_v} demonstrates the effectiveness of via detection and grouping technique. With via detection 
	technique, the 
	number of nodes which will be stamped into the system conductance matrix is decreased by nearly 2/3. Comparing $t_N$ 
	and $t_{Rep}$, we can see that there is a
	great reduce in simulation time. Adding grouping technique further decreases the simulation time by half. If the 3 components are 
	similar in size, even faster simulation time can be achieved.

	Table \ref{tb_s}, \ref{tb_m} exhibits the obvious advantage of the proposed method. There is a tremendous 
	speedup over direct LU solver. For each benchmark, there is at least tens of speedup. The biggest speedup is over 110X. 

	Table \ref{tb_l} shows that the proposed method successfully finishes the simulation of large-scale power grids
	in a short time, while LU can not handle the problem, due to its large size. In the experiment, the largest problem that LU 
	can solve is around 8M nodes in each component. 

	It should be noticed that since each processor in the distributed memory system has smaller memory size, grid needs to be 
	partitioned into enough subdomains so that each subdomain is small enough to be loaded in memory. Because each processor handles 
	one 
	subdomain, enough processors will be needed. For large-scale benchmarks, if there is sufficient processor resource, several 
	components can be solved simultaneously. Otherwise, those components should be 
	solved sequentially. In Table \ref{tb_l}, netlist\_32M, netlist\_72M and netlist\_
	108M are 
	simulated with components being solved simultaneously. For netlist\_144M and 
	netlist\_192M, as not enough processors can get allocated, components are solved sequentially.

\section{Conclusions}
	This paper presents detail implementation of an efficient parallel ASM for DC power grid analysis. Based on distribute memory 
	system, a new data storage strategy is proposed to efficiently handle the large problem size. An overlapping technique for 3D 
	irregular grid friendly introduced to accelerate the convergence. A new method is presented to achieve the minimum communication overhead. Via 
	detection and grouping technique are adopted to decrease the problem size, and introduce more parallelism. Experimental 
	results of several industrial benchmarks show that more than 110X speedup are gained over a state-of-art LU solver. It is also 
	the first time reported that benchmark as large as 192M can be handled and finished within 5 minutes. With enough processors, 
	even larger benchmarks can be simulated.

\bibliographystyle{abbrv}
\bibliography{sig-alternate}
\end{document}
